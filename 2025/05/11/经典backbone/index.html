<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#2d3748">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#2d3748">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lizhulan.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":300,"display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#667eea","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="经典backboneAlexNet创新  首次使用GPU训练网络 使用Relu激活函数 使用LRN局部响应归一化 在全连接层加入Dropout  LRN的主要思想是在神经元输出的局部范围内进行归一化操作，使得激活值较大的神经元对后续神经元的影响降低，从而减少梯度消失和梯度爆炸的问题。  为什么说归一化的目的可以被理解为“抑制”？ 虽然归一化的主要目的是调整数据的范围和分布，但可以从某种角度将其与“">
<meta property="og:type" content="article">
<meta property="og:title" content="经典backbone">
<meta property="og:url" content="https://lizhulan.github.io/2025/05/11/%E7%BB%8F%E5%85%B8backbone/index.html">
<meta property="og:site_name" content="李猪兰">
<meta property="og:description" content="经典backboneAlexNet创新  首次使用GPU训练网络 使用Relu激活函数 使用LRN局部响应归一化 在全连接层加入Dropout  LRN的主要思想是在神经元输出的局部范围内进行归一化操作，使得激活值较大的神经元对后续神经元的影响降低，从而减少梯度消失和梯度爆炸的问题。  为什么说归一化的目的可以被理解为“抑制”？ 虽然归一化的主要目的是调整数据的范围和分布，但可以从某种角度将其与“">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c23a0ebceb9.png">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c23b35928e1.png">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c251c5408cb.png">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c254ae99a9d.png">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c254daf0e90.png">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c255abc4aa0.png">
<meta property="og:image" content="https://free.picui.cn/free/2025/09/11/68c2596129090.png">
<meta property="article:published_time" content="2025-05-11T05:20:39.000Z">
<meta property="article:modified_time" content="2025-09-11T05:20:51.000Z">
<meta property="article:author" content="李猪兰">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="图像处理">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://free.picui.cn/free/2025/09/11/68c23a0ebceb9.png">

<link rel="canonical" href="https://lizhulan.github.io/2025/05/11/%E7%BB%8F%E5%85%B8backbone/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>经典backbone | 李猪兰</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">李猪兰</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-docs">

    <a href="/docs/" rel="section"><i class="fa fa-book fa-fw"></i>docs</a>

  </li>
        <li class="menu-item menu-item-plugins">

    <a href="/plugins/" rel="section"><i class="fa fa-plug fa-fw"></i>plugins</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">11</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">20</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/lizhulan" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lizhulan.github.io/2025/05/11/%E7%BB%8F%E5%85%B8backbone/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="李猪兰">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李猪兰">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          经典backbone
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-05-11 13:20:39" itemprop="dateCreated datePublished" datetime="2025-05-11T13:20:39+08:00">2025-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-11 13:20:51" itemprop="dateModified" datetime="2025-09-11T13:20:51+08:00">2025-09-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="经典backbone"><a href="#经典backbone" class="headerlink" title="经典backbone"></a>经典backbone</h1><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p><strong>创新</strong></p>
<ol>
<li>首次使用GPU训练网络</li>
<li>使用Relu激活函数</li>
<li>使用LRN局部响应归一化</li>
<li>在全连接层加入Dropout</li>
</ol>
<p><strong>LRN的主要思想是在神经元输出的局部范围内进行归一化操作，使得激活值较大的神经元对后续神经元的影响降低，从而减少梯度消失和梯度爆炸的问题。</strong></p>
<blockquote>
<p><strong>为什么说归一化的目的可以被理解为“抑制”？</strong></p>
<p>虽然归一化的主要目的是调整数据的范围和分布，但可以从某种角度将其与“抑制”联系起来：</p>
<ul>
<li><strong>抑制异常值的影响</strong>：在归一化过程中，极端值（异常值）会被调整到与其他值相近的范围内，从而减少其对模型训练的负面影响。这种调整可以被看作是对异常值的“抑制”。</li>
<li><strong>抑制某些特征的主导作用</strong>：如果某些特征的值域远大于其他特征，它们可能会在模型训练中占据主导地位。归一化通过将所有特征调整到相同的范围，抑制了这些特征的过度影响，使模型能够更公平地处理所有特征。</li>
<li><strong>抑制过大的梯度</strong>：在神经网络中，归一化可以防止某些神经元的激活值过大，从而避免梯度爆炸问题。这种对激活值的调整也可以被看作是一种“抑制”。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>Dropout为什么能够解决过拟合</strong>：</p>
<p>（1）<strong>减少过拟合：</strong><br>在标准的神经网络中，网络可能会过度依赖于一些特定的神经元，导致对训练数据的过拟合。Dropout通过随机丢弃神经元，迫使网络学习对于任何单个神经元的变化都要更加鲁棒的特征表示，从而减少了对训练数据的过度拟合。</p>
<p>（2）<strong>取平均的作用：</strong><br>在训练过程中，通过丢弃随机的神经元，每次前向传播都相当于在训练不同的子网络。在测试阶段，不再进行Dropout，但是通过保留所有的权重，网络结构变得更加完整。因此，可以看作是在多个不同的子网络中进行了训练，最终的预测结果相当于对这些子网络的输出取平均。这种“综合取平均”的策略有助于减轻过拟合，因为一些互为反向的拟合会相互抵消。</p>
</blockquote>
<h2 id="VggNet"><a href="#VggNet" class="headerlink" title="VggNet"></a>VggNet</h2><p><strong>VGGNet的特点是在训练过程中采用小的卷积核和池化层，并通过堆叠多个卷积层来增加网络的深度，从而实现高精度的图像识别。</strong><br><strong>VGGNet的亮点</strong>在于它通过堆叠多个卷积层，以小的卷积核和池化层的方式来增加网络深度，从而实现高精度的图像识别。这种方法可以有效地捕获图像中的高级特征，并通过不断拟合训练数据来提高识别准确率。<br><strong>感受野</strong><br>感受野（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。</p>
<p>卷积神经网络中，越深层的神经元看到的输入区域越大，如下图所示，卷积核kernel size 均为3×3，stride均为1，绿色标记的是Layer2绿色神经元看到的区域，黄色标记的是Layer3 看到的区域，具体地，Layer2每个神经元可看到Layer1上3×3 大小的区域，Layer3 每个神经元看到Layer2 上3×3 大小的区域，该区域可以又看到Layer1上5×5 大小的区域。<br><img src="https://free.picui.cn/free/2025/09/11/68c23a0ebceb9.png" alt="输入图片说明"><br>感受野是个相对概念，某层feature map上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。<br>堆叠<strong>两个3x3的卷积核可以替代5x5的卷积核，堆叠三个3x3的卷积核可以替代7x7的卷积核</strong>，那么通过多个小的感受野的堆叠，替代大的感受野是为了减少网络训练时参数的数量，<strong>堆叠三个3x3的卷积核的计算量要小于7x7的卷积核，而且这个差距会随着特征矩阵的channels的变大而扩大</strong></p>
<blockquote>
<p>尽管VggNet在许多方面都表现优秀，但它也有一些缺陷：</p>
<ol>
<li><p>该网络架构非常大，并且需要大量的计算资源来训练。这意味着，如果你想在较小的设备上使用VggNet，比如移动设备或个人电脑，会发现它非常慢，并且可能无法获得足够的性能。</p>
</li>
<li><p>由于VggNet网络架构非常深，它可能会导致梯度消失或爆炸的问题。这是由于在非常深的神经网络中，梯度在传播过程中可能会变得非常小或非常大，从而导致模型无法正常训练。</p>
</li>
</ol>
</blockquote>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p><strong>创新</strong></p>
<ol>
<li>Inception结构</li>
<li>1*1卷积核降维</li>
<li>辅助分类器</li>
</ol>
<h3 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h3><p><img src="https://free.picui.cn/free/2025/09/11/68c23b35928e1.png" alt="输入图片说明"></p>
<blockquote>
<p>Inception结构是对输入图像并行地执行多个卷积运算或池化操作，并将所有输出结果拼接为一个非常深的特征图，且不同大小卷积核的卷积运算可以得到图像中的不同信息，处理获取到的图像中的不同信息可以得到更好的图像特征</p>
</blockquote>
<p>Inception结构通常用于图像分类和识别任务，因为它能够有效地捕捉图像中的细节信息。它的主要优势在于能够以高效的方式处理大量的数据，并且模型的参数量相对较少，这使得它能够在不同的设备上运行。</p>
<blockquote>
<p><strong>Inception 模块的设计基于以下两个核心思想</strong>：</p>
<ol>
<li><p><strong>多尺度特征提取</strong>：通过并行的多个卷积分支，Inception 模块能够同时捕捉不同尺度的特征。这使得网络能够更好地处理图像中的多尺度信息，例如小目标和大目标。</p>
<ul>
<li><p><strong>1×1 卷积分支</strong>：用于提取局部特征，同时减少通道数，起到降维的作用。</p>
</li>
<li><p><strong>3×3 卷积分支</strong>：用于捕捉中等尺度的特征。</p>
</li>
<li><p><strong>5×5 卷积分支</strong>：用于捕捉较大尺度的特征。</p>
</li>
<li><p><strong>池化分支</strong>：通常是一个 3×3 的最大池化或平均池化，用于捕捉更全局的特征。</p>
</li>
</ul>
</li>
<li><p><strong>计算资源优化</strong>：通过合理分配计算资源，Inception 模块在不显著增加计算量的情况下，增加了网络的深度和宽度。这使得网络能够在有限的计算资源下达到更高的性能</p>
</li>
</ol>
</blockquote>
<h3 id="1x1卷积"><a href="#1x1卷积" class="headerlink" title="1x1卷积"></a>1x1卷积</h3><p> 1x1卷积核，又称为网中网（Network in Network）。<br>其实1x1卷积，可以看成一种全连接（full connection）。</p>
<h3 id="辅助分类器"><a href="#辅助分类器" class="headerlink" title="辅助分类器"></a>辅助分类器</h3><p>GoogLeNet（Inception v1）是一个22层的深层网络，在2014年提出时面临两个关键挑战：</p>
<ol>
<li><p><strong>梯度消失</strong>：深层网络中反向传播时梯度逐渐衰减</p>
</li>
<li><p><strong>训练不稳定</strong>：深层参数难以有效更新</p>
</li>
</ol>
<p>解决方案：引入辅助分类器（Auxiliary Classifiers），通过中间监督信号改善训练</p>
<p><strong>辅助分类器主要是为了解决训练深层网络时遇到的梯度消失和过拟合问题 。</strong><br><strong>辅助分类器的作用</strong></p>
<ul>
<li><p><strong>缓解梯度消失</strong>：由于网络深度达22层，反向传播时梯度可能在前几层衰减严重。辅助分类器通过中间层的额外损失计算，直接向浅层传递梯度信号，加速训练收敛28。</p>
</li>
<li><p><strong>正则化效果</strong>：辅助分类器引入的额外损失（权重为0.3）对主分类器起到正则化作用，减少过拟合风险。</p>
</li>
<li><p><strong>特征融合</strong>：中间层的特征具有较强判别能力，辅助分类器通过多尺度特征融合提升模型表达能力。</p>
</li>
</ul>
<p>辅助分类器通常与主要的分类器结合使用，以帮助模型更好地理解图像中的细节和复杂模式。这种技术可以提高模型的泛化能力，使其更准确地预测未知的图像，在GoogLeNet中有两个辅助分类器，分别在<strong>Inception4a和Inception4d</strong></p>
<blockquote>
<p>尽管GoogLeNet在当时取得了很好的成绩，但它也有一些缺点。</p>
<ol>
<li><p>由于其使用了Inception模块，网络的计算复杂度较高。这使得GoogLeNet的训练速度较慢，不太适合对实时性要求较高的应用。</p>
</li>
<li><p>GoogLeNet的网络结构相对复杂，不太容易理解，并且由于辅助分类器的存在，这使得在调试和优化网络时较为困难。</p>
</li>
</ol>
<p>总之，GoogLeNet的计算复杂度高、网络结构复杂是其主要缺点。</p>
</blockquote>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>ResNet的主要特点是采用了残差学习机制。在传统的神经网络中，每一层的输出都是直接通过一个非线性激活函数得到的。但在ResNet中，每一层的输出是通过一个“残差块”得到的，该残差块包含了一个快捷连接（shortcut）和几个卷积层。这样，在训练过程中，每一层只需要<strong>学习残差</strong>（即输入与输出之间的差异），而不是所有的信息。这<strong>有助于防止梯度消失和梯度爆炸</strong>的问题，从而使得网络能够训练得更深。<br>ResNet的主要优点是具有非常深的层数，可以达到1000多层，但仍然能够高效地训练。这是通过使用残差连接来实现的，这种连接允许模型学习跨越多个层的残差，而不是直接学习每一层的输出。这使得ResNet能够更快地收敛，并且能够更好地泛化到新的数据集，ResNet论文中共提出了五种结构，分别是ResNet-18，ResNet-34，ResNet-50，ResNet-101，ResNet-152。<br><strong>创新</strong></p>
<ol>
<li>Residual结构</li>
<li>Batch Normalization</li>
</ol>
<h3 id="Residual结构"><a href="#Residual结构" class="headerlink" title="Residual结构"></a>Residual结构</h3><p>Residual结构是残差结构，在文章中给了两种不同的残差结构，在ResNet-18和ResNet-34中，用的如下图中左侧图的结构，在ResNet-50、ResNet-101和ResNet-152中，用的是下图中右侧图的结构。<br><img src="https://free.picui.cn/free/2025/09/11/68c251c5408cb.png" alt="输入图片说明"><br>在主分支上有一个圆弧的线从输入特征矩阵直接连到了一个加号，这个圆弧的线是shortcut（捷径分支），它直接将输入特征矩阵加到经过第二次3x3的卷积核卷积之后的输出特征矩阵，注意，这里描述的是加，而不是叠加或者拼接，也就是说是矩阵对应维度位置进行一个加法运算，意味着主分支的输出矩阵和shortcut的输出矩阵的shape必须相同（此时卷积默认padding&#x3D;1），这里包括宽、高、channels，在相加之后，再经过Relu激活函数进行激活。<br>为什么**残差结构能够抑制degradation problem(退化)**问题呢？<br>其实残差的网络结构很简单：$H(x)&#x3D;F(x)+x$，其中，$x$是shortcut的输出特征矩阵，在某些残差结构中它就是输入特征矩阵，$F(x)$是主分支的输出特征矩阵，假如没有残差网络的话直接将输入特征矩阵$x$放入卷积层中进行计算来得到$H(x)$，如果加入残差结构的话，输入特征矩阵$x$进入神经网络得到也就等同于$H(x)-x$，在与$x$相加得到$H(x)$。</p>
<p>根据$H(x)&#x3D;F(x)+x$，可以求得${\frac{\partial H(x)}{\partial w}&#x3D;{\frac{\partial F(x)}{\partial w}}+{\frac{\partial x}{\partial w}}}$，可以知道本次残差结构的反向传播不仅与主分支$F(x)$有关，还与$x$有关，当经过主分支的卷积层的导数很小的情况下，那么会对网络起不到更新的作用，但是由于加入了输入矩阵的导数，也就保证了网络导数会一直存在，而不会出现导数消失的情况。<br><strong>ResNet网络中的残差结构的堆叠有点这类似于这个过程</strong>：刚开始的前几个残差结构提取了很重要的一些特征，后几个残差结构负责把一些特征进行细化，当然，后几个残差结构可能有的会学不到东西，但是并不会影响前面的梯度从而造成degradation problem退化问题。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Batch Normalization的作用是将一个批次（Batch）的特征矩阵的每一个channels（通道）计算为均值为0，方差为1的分布规律。<br>一般而言，在一个神经网络输入图像之前，会将图像进行预处理，这个预处理可能是标准化处理等手段，由于输入数据满足某一分布规律，所以会加速网络的收敛。这样在输入第一次卷积的时候满足某一分布规律，但是在输入第二次卷积时，就不一定满足某一分布规律了，再往后的卷积的输入就更不满足了，那么就需要一个中间商，让上一层的输出经过它之后能够某一分布规律，Batch Normalization就是这个中间商，它可以让输入的特征矩阵的每一个channels满足均值为0，方差为1的分布规律。</p>
<blockquote>
<p>用<strong>Batch Normalization要注意的问题</strong>：</p>
<ol>
<li><p><strong>Batch Size要设置的大一些</strong>，Batch Size越大，均值和方差越接近整个数据集的均值和方差，所以这对GPU显存比较小的机器不是很友好，个人建议要≥4。</p>
</li>
<li><p>ResNet文章中提到BN层要放在卷积和激活之间，并且卷积层不要有偏置，因为有没有偏置都计算出来的结果都是一致的：</p>
<ul>
<li><p>没有偏置的公式为：$y_{i}&#x3D;{\frac{x_{i}-\mu(x)}{\sqrt{\sigma^{2}(x)}}}$，先放在这里，待会直接做对比。</p>
</li>
<li><p>有偏置的公式为：$y_{i}^{b},&#x3D;,\frac{x_{i}^{b}!-!\mu(x^{b})}{\sqrt{\sigma^{2}(x^{b})}}$，其中$x_{i}^{b}&#x3D;x_{i}+b$；</p>
<ul>
<li><p>对于均值而言：$\mu(x^{b})&#x3D;\mu(x)+b$；</p>
</li>
<li><p>对于方差而言：$\sigma^{2}(x^{b})&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}[x_{i}^{b}-\mu(x^{b})]^{2}&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}[x_{i}+b-\mu(x)-b]^{2}&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}[x_{i}-\mu(x)]^{2}&#x3D;\sigma^{2}(x)$，</p>
</li>
<li><p>那么可以计算出：$y_{i}^{b}&#x3D;\frac{x_{i}^{b}-\mu(x^{b})}{\sqrt{\sigma^{2}(x^{b})}}&#x3D;\frac{x_{i}+b-\mu(x)-b}{\sqrt{\sigma^{2}(x)}}&#x3D;\frac{x_{i}-\mu(x)}{\sqrt{\sigma^{2}(x)}}$。</p>
</li>
<li><p>可以看出来，这个值和没有偏置的时候的$y_{i}&#x3D;{\frac{x_{i}-\mu(x)}{\sqrt{\sigma^{2}(x)}}}$是完全一致的。</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p>由于ResNet的结构非常复杂，所以它的训练时间比较长。此外，由于它具有非常深的层数，因此它需要大量的数据来进行训练。</p>
</blockquote>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><h3 id="V1"><a href="#V1" class="headerlink" title="V1"></a>V1</h3><p><strong>创新</strong></p>
<ol>
<li>Depthwise Convolution</li>
<li>$\alpha、\beta$ 的引入</li>
</ol>
<h4 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a>Depthwise Convolution</h4><p>DW卷积（Depthwise Convolution），它能够大大减少模型的参数以及运算量。<br><strong>经典的卷积</strong><br><img src="https://free.picui.cn/free/2025/09/11/68c254ae99a9d.png" alt="输入图片说明"><br><strong>DW卷积</strong><br><img src="https://free.picui.cn/free/2025/09/11/68c254daf0e90.png" alt="输入图片说明"><br>DW卷积的每一个卷积核负责一个输入特征矩阵的channel，那么总结下来：输入特征矩阵的channel &#x3D; 卷积核个数 &#x3D; 输出特征矩阵的channel。<br><strong>深度可分卷积操作（Depthwise Separable Conv</strong><br><a target="_blank" rel="noopener" href="https://lizhuoran.com/2025/09/10/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/">深度可分离卷积 | 李猪兰</a><br>这种卷积操作是有两种卷积组成的，即DW卷积和PW卷积（Pointwise Conv）组成，先DW卷积，结果作为PW卷积的输入<br><strong>PW卷积</strong><br><img src="https://free.picui.cn/free/2025/09/11/68c255abc4aa0.png" alt="输入图片说明"><br>通常情况下，DW卷积核PW卷积是放在一起使用的，这种卷积比普通的卷积能节省大量的计算量，由上图普通卷积、DW卷积、PW卷积三个图可知，普通卷积的输入是3通道，输出是4通道，深度可分卷积由DW和PW一起构成（先DW后PW），所以输入是3通道，输出是4通道。</p>
<blockquote>
<p>在使用过程中，发现了部分DW卷积核会废掉的情况，即卷积核的参数为0，为什么会出现这种问题呢？主要是以下几个原因：</p>
<ol>
<li><p>卷积核、通道数量以及权重数量太少，感受野太单薄；</p>
</li>
<li><p>Relu激活函数；</p>
</li>
<li><p>精度低时，例如float18，int8这种低精度浮点数表示时很有限。</p>
</li>
</ol>
<p>这种情况会造成特征拿不全的问题，但是这个问题在MobileNetV2中得以改善。</p>
</blockquote>
<h4 id="alpha、-beta"><a href="#alpha、-beta" class="headerlink" title="$\alpha、\beta$"></a>$\alpha、\beta$</h4><p>$\alpha$ 是卷积核个数的倍率，用来控制卷积过程中卷积核的个数，当取不同的$\alpha$的时候，准确率、计算量和参数量是不一样的<br>$\beta$ 是分辨率参数，即输入图像尺寸的参数</p>
<p>这种情况会造成特征拿不全的问题，但是这个问题在MobileNetV2中得以改善。</p>
<h3 id="V2"><a href="#V2" class="headerlink" title="V2"></a>V2</h3><p><strong>创新</strong></p>
<ol>
<li>Inverted Residuals（倒残差结构）</li>
<li>Relu6</li>
<li>Linear Bottlenecks</li>
<li>Shortcut</li>
<li>拓展因子</li>
</ol>
<h4 id="Inverted-Residuals（倒残差结构）"><a href="#Inverted-Residuals（倒残差结构）" class="headerlink" title="Inverted Residuals（倒残差结构）"></a>Inverted Residuals（倒残差结构）</h4><p>在之前的ResNet残差结构是先用1x1的卷积降维，再升维的操作。而在MobileNetV2中，是先升维，再降维的操作，所以该结构叫倒残差结构，网络结构表格中的bottleneck就是倒残差结构。</p>
<blockquote>
<p><strong>残差结构的过程是</strong>：</p>
<ol>
<li><p>1x1卷积降维</p>
</li>
<li><p>3x3卷积</p>
</li>
<li><p>1x1卷积升维</p>
</li>
</ol>
<p>即对输入特征矩阵进行利用1x1卷积进行降维，减少输入特征矩阵的channel，然后通过3x3的卷积核进行处理提取特征，最后通过1x1的卷积核进行升维，那么它的结构就是两边深，中间浅的结构</p>
</blockquote>
<blockquote>
<p><strong>倒残差结构的过程是</strong>：</p>
<ol>
<li><p>首先会通过一个1x1卷积层来进行升维处理，在卷积后会跟有BN和Relu6激活函数</p>
</li>
<li><p>紧接着是一个3x3大小DW卷积，卷积后面依旧会跟有BN和Relu6激活函数</p>
</li>
<li><p>最后一个卷积层是1x1卷积，起到降维作用，注意卷积后只跟了BN结构，并没有使用Relu6激活函数。</p>
</li>
</ol>
<p>结构应该是中间深，两边浅</p>
</blockquote>
<h4 id="Relu6"><a href="#Relu6" class="headerlink" title="Relu6"></a>Relu6</h4><p>$y&#x3D;ReLU6(x)&#x3D;min(max(x,0),6)$</p>
<p>Relu6激活函数是Relu的变种，它的输入小于0时，结果为0，输入大于6时：结果为6。在0到6之间时：$y&#x3D;x$。</p>
<blockquote>
<p>如果对Relu的激活范围不加限制，输出范围为0到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的float16无法很好地精确描述如此大范围的数值，带来精度损失，所以在量化过程中，Relu6能够有更好的量化表现和更小的精度下降。</p>
</blockquote>
<h4 id="Linear-Bottlenecks"><a href="#Linear-Bottlenecks" class="headerlink" title="Linear Bottlenecks"></a>Linear Bottlenecks</h4><p> Linear Bottlenecks（线性瓶颈）</p>
<h4 id="Shortcut"><a href="#Shortcut" class="headerlink" title="Shortcut"></a>Shortcut</h4><p>Shortcut并不是该网络提出的，而是残差结构提出的。<br><img src="https://free.picui.cn/free/2025/09/11/68c2596129090.png" alt="输入图片说明"><br>shortcut将输入与输出直接进行相加，可以使得网络在较深的时候依旧可以进行训练。</p>
<h3 id="V3"><a href="#V3" class="headerlink" title="V3"></a>V3</h3><p><strong>创新</strong></p>
<ol>
<li>更新Block</li>
<li>使用NAS搜索参数</li>
<li>重新设计耗时层</li>
</ol>
<p>NAS是Neural Architecture Search，实现网络结构搜索<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/450940875">https://zhuanlan.zhihu.com/p/450940875</a>。</p>
<blockquote>
<p>在使用过程中，可能会出现MobileNetV3准确率不如MobileNetV2的情况，MobileNetV3是来自于Google的，自然它更关注的是网络在Android设备上的表现，事实也的确如此，作者主要针对Google<br>Pixel硬件对网络做了参数优化。</p>
<p>当然这并不意味着MobileNet V3就是慢的了，只不过它无法在其他一些设备上达到最佳效果。</p>
</blockquote>
<h3 id="V4"><a href="#V4" class="headerlink" title="V4"></a>V4</h3><p><strong>创新</strong></p>
<ol>
<li>Universal Inverted Bottleneck（UIB）——统一可搜索的构建块</li>
<li>Mobile MQA——为移动加速器定制的注意力机制</li>
<li>两阶段NAS配方——硬件感知的结构搜索</li>
<li>蒸馏增强训练——精度再提升</li>
</ol>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>密集连接，<strong>特征复用</strong>强，参数效率高</p>
<h2 id="SENet（Squeeze-and-Excitation）"><a href="#SENet（Squeeze-and-Excitation）" class="headerlink" title="SENet（Squeeze-and-Excitation）"></a>SENet（Squeeze-and-Excitation）</h2><p>引入 <strong>通道注意力机制</strong>，可嵌入其他网络提升性能</p>
<h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><p>ResNet 的升级版，引入 <strong>分组卷积</strong>，性能更强</p>
<h2 id="ShuffleNet-V1-V2"><a href="#ShuffleNet-V1-V2" class="headerlink" title="ShuffleNet V1&#x2F;V2"></a>ShuffleNet V1&#x2F;V2</h2><p>轻量级网络，<strong>通道打乱机制</strong>，适合移动端</p>
<h2 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h2><p><strong>复合缩放</strong>策略，精度高、参数少，<strong>EfficientDet 的 backbone</strong></p>
<h2 id="RegNet"><a href="#RegNet" class="headerlink" title="RegNet"></a>RegNet</h2><p>Facebook 提出，<strong>系统设计网络结构</strong>，Detectron2 默认使用</p>
<h2 id="Vision-Transformer-ViT"><a href="#Vision-Transformer-ViT" class="headerlink" title="Vision Transformer (ViT)"></a>Vision Transformer (ViT)</h2><p>使用 Transformer 架构，<strong>无卷积</strong>，在大规模数据上表现强劲</p>
<h2 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h2><p>分层 Transformer，<strong>兼容 CNN 结构</strong>，在检测&#x2F;分割任务中表现优异</p>
<h2 id="怎么选择"><a href="#怎么选择" class="headerlink" title="怎么选择"></a>怎么选择</h2><p><strong>按类型分类</strong></p>
<table>
<thead>
<tr>
<th>类型</th>
<th>代表模型</th>
</tr>
</thead>
<tbody><tr>
<td><strong>标准 CNN</strong></td>
<td>AlexNet, VGG, GoogLeNet, ResNet, DenseNet, ResNeXt</td>
</tr>
<tr>
<td><strong>轻量级</strong></td>
<td>MobileNet V1&#x2F;V2&#x2F;V3, ShuffleNet V1&#x2F;V2, EfficientNet</td>
</tr>
<tr>
<td><strong>注意力机制</strong></td>
<td>SENet, CBAM（可嵌入其他网络）</td>
</tr>
<tr>
<td><strong>Transformer 系列</strong></td>
<td>ViT, Swin Transformer, DeiT</td>
</tr>
</tbody></table>
<p><strong>按用途分类</strong></p>
<h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a><strong>目标检测</strong></h3><table>
<thead>
<tr>
<th>任务场景</th>
<th>推荐 backbone</th>
<th>理由</th>
<th>常用组合</th>
</tr>
</thead>
<tbody><tr>
<td><strong>高精度</strong><br>服务器&#x2F;GPU</td>
<td>ResNet-50-FPN<br>ResNet-101-FPN</td>
<td>鲁棒、预训练多、FPN 天生适配</td>
<td>Faster R-CNN、Mask R-CNN、RetinaNet</td>
</tr>
<tr>
<td><strong>轻量级</strong><br>移动端</td>
<td>MobileNet-V3-Large-FPN</td>
<td>参数量 ↓90%，仍保留 FPN 结构</td>
<td>SSD-Lite、YOLOv4-tiny、EfficientDet-D0</td>
</tr>
<tr>
<td><strong>超高精度</strong><br>比赛&#x2F;科研</td>
<td>Swin-Tiny-FPN<br>RegNetY-4GF-FPN</td>
<td>分层 Transformer &#x2F; 设计空间搜索</td>
<td>Detectron2、MMDetection 一键调用</td>
</tr>
</tbody></table>
<h3 id="语义分割-实例分割"><a href="#语义分割-实例分割" class="headerlink" title="语义分割 &#x2F; 实例分割"></a><strong>语义分割 &#x2F; 实例分割</strong></h3><table>
<thead>
<tr>
<th>任务场景</th>
<th>推荐 backbone</th>
<th>理由</th>
<th>常用组合</th>
</tr>
</thead>
<tbody><tr>
<td><strong>通用</strong></td>
<td>ResNet-50-DeepLabV3+<br>ResNet-101-DeepLabV3+</td>
<td>空洞卷积 + ASPP，成熟 pipeline</td>
<td>DeepLabV3+、U-Net++</td>
</tr>
<tr>
<td><strong>实时</strong></td>
<td>MobileNet-V2<br>ShuffleNet-V2</td>
<td>速度 30+ FPS@720p， TensorRT 友好</td>
<td>BiSeNet、SwiftNet</td>
</tr>
<tr>
<td><strong>高精度</strong></td>
<td>Swin-Tiny-UperNet<br>RegNetY-8GF-MaskFormer</td>
<td>全局上下文强，MaskFormer 统一分割</td>
<td>MMSeg、Detectron2</td>
</tr>
</tbody></table>
<h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a><strong>微调</strong></h3><table>
<thead>
<tr>
<th>数据量</th>
<th>推荐 backbone</th>
<th>技巧</th>
</tr>
</thead>
<tbody><tr>
<td><strong>千级</strong></td>
<td>VGG-16 &#x2F; DenseNet-121</td>
<td>冻结前 1&#x2F;3 层 + 小学习率 1e-4</td>
</tr>
<tr>
<td><strong>万级</strong></td>
<td>ResNet-50 &#x2F; EfficientNet-B0</td>
<td>全局微调 + RandAugment</td>
</tr>
<tr>
<td><strong>百级</strong></td>
<td>Swin-Tiny</td>
<td>冻结 patch embedding，只训 stages 3-4</td>
</tr>
</tbody></table>
<p>总结</p>
<ul>
<li><p><strong>服务器精度</strong> → <strong>ResNet-FPN</strong> 永远是 baseline；</p>
</li>
<li><p><strong>移动端速度</strong> → <strong>MobileNet-V3</strong> 先跑通，再试 <strong>ShuffleNet-V2</strong>；</p>
</li>
<li><p><strong>Transformer 尝鲜</strong> → <strong>Swin-Tiny</strong> 最小可用，<strong>RegNet</strong> 调参最省心；</p>
</li>
<li><p><strong>边缘量化</strong> → 选 <strong>无 SE 模块</strong> 的版本（SE 对 INT8 不友好）。</p>
</li>
</ul>
<p>（来自ai）</p>

    </div>

    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      </div>
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>李猪兰
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://lizhulan.github.io/2025/05/11/%E7%BB%8F%E5%85%B8backbone/" title="经典backbone">https://lizhulan.github.io/2025/05/11/经典backbone/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>




      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
              <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" rel="tag"># 图像处理</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/05/10/OpenCV/" rel="prev" title="OpenCV">
      <i class="fa fa-chevron-left"></i> OpenCV
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/07/09/%E6%8F%90%E7%A4%BA%E8%AF%8D/" rel="next" title="提示词">
      提示词 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8backbone"><span class="nav-number">1.</span> <span class="nav-text">经典backbone</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">1.1.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VggNet"><span class="nav-number">1.2.</span> <span class="nav-text">VggNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">1.3.</span> <span class="nav-text">GoogLeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception%E7%BB%93%E6%9E%84"><span class="nav-number">1.3.1.</span> <span class="nav-text">Inception结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1x1%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.3.2.</span> <span class="nav-text">1x1卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%85%E5%8A%A9%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.3.3.</span> <span class="nav-text">辅助分类器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">1.4.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual%E7%BB%93%E6%9E%84"><span class="nav-number">1.4.1.</span> <span class="nav-text">Residual结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">1.4.2.</span> <span class="nav-text">Batch Normalization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNet"><span class="nav-number">1.5.</span> <span class="nav-text">MobileNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#V1"><span class="nav-number">1.5.1.</span> <span class="nav-text">V1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Depthwise-Convolution"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">Depthwise Convolution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#alpha%E3%80%81-beta"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">$\alpha、\beta$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V2"><span class="nav-number">1.5.2.</span> <span class="nav-text">V2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inverted-Residuals%EF%BC%88%E5%80%92%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84%EF%BC%89"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">Inverted Residuals（倒残差结构）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Relu6"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">Relu6</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Bottlenecks"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">Linear Bottlenecks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shortcut"><span class="nav-number">1.5.2.4.</span> <span class="nav-text">Shortcut</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V3"><span class="nav-number">1.5.3.</span> <span class="nav-text">V3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#V4"><span class="nav-number">1.5.4.</span> <span class="nav-text">V4</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseNet"><span class="nav-number">1.6.</span> <span class="nav-text">DenseNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SENet%EF%BC%88Squeeze-and-Excitation%EF%BC%89"><span class="nav-number">1.7.</span> <span class="nav-text">SENet（Squeeze-and-Excitation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNeXt"><span class="nav-number">1.8.</span> <span class="nav-text">ResNeXt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ShuffleNet-V1-V2"><span class="nav-number">1.9.</span> <span class="nav-text">ShuffleNet V1&#x2F;V2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EfficientNet"><span class="nav-number">1.10.</span> <span class="nav-text">EfficientNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RegNet"><span class="nav-number">1.11.</span> <span class="nav-text">RegNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vision-Transformer-ViT"><span class="nav-number">1.12.</span> <span class="nav-text">Vision Transformer (ViT)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Swin-Transformer"><span class="nav-number">1.13.</span> <span class="nav-text">Swin Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9"><span class="nav-number">1.14.</span> <span class="nav-text">怎么选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-number">1.14.1.</span> <span class="nav-text">目标检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="nav-number">1.14.2.</span> <span class="nav-text">语义分割 &#x2F; 实例分割</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83"><span class="nav-number">1.14.3.</span> <span class="nav-text">微调</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="李猪兰"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">李猪兰</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/lizhulan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lizhulan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lizhulan@example.com" title="E-Mail → mailto:lizhulan@example.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/docs/" title="Docs → &#x2F;docs&#x2F;"><i class="fa fa-book fa-fw"></i>Docs</a>
      </span>
      <span class="links-of-author-item">
        <a href="/plugins/" title="Plugins → &#x2F;plugins&#x2F;"><i class="fa fa-plug fa-fw"></i>Plugins</a>
      </span>
      <span class="links-of-author-item">
        <a href="/archives/" title="Archives → &#x2F;archives&#x2F;"><i class="fa fa-archive fa-fw"></i>Archives</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2024 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李猪兰</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">101k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:32</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共54.6k字</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script color='102,126,234' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script size="300" alpha="0.3" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
